1) 
N_samples = 128
U_net_shape = 1-4-8-16-32-64
batch_size = 16
split_ratio = 0.75
learning_rate = 0.001

2)
N_samples = 128
U_net_shape = 1-4-8-16-32-64
batch_size = 8
split_ratio = 0.75
learning_rate = 0.001

3)
N_samples = 128
U_net_shape = 1-4-8-16-32-64
batch_size = 4
split_ratio = 0.75
learning_rate = 0.001

4)
N_samples = 128
U_net_shape = 1-4-8-16-32-64
batch_size = 8
split_ratio = 0.75
learning_rate = 0.1
FAIL

5)
N_samples = 128
U_net_shape = 1-4-8-16-32-64
batch_size = 8
split_ratio = 0.75
learning_rate = 0.01
FAIL

6)
N_samples = 128
U_net_shape = 1-4-8-16-32-64
batch_size = 8
split_ratio = 0.75
learning_rate = 0.005

7)
N_samples = 128
U_net_shape = 1-4-8-16-32-64
batch_size = 8
split_ratio = 0.875
learning_rate = 0.001

8)
N_samples = 256
U_net_shape = 1-4-8-16-32-64
batch_size = 16
split_ratio = 0.75
learning_rate = 0.001

9)
N_samples = 256
U_net_shape = 1-4-8-16-32-64
batch_size = 8
split_ratio = 0.75
learning_rate = 0.001
files used: data_elasticity_3D_128_0.pt
data_elasticity_3D_128_1.pt

10)
N_samples = 384
U_net_shape = 1-4-8-16-32-64
batch_size = 8
split_ratio = 0.75
learning_rate = 0.001
files used: data_elasticity_3D_128_0.pt
data_elasticity_3D_128_1.pt
data.pt

11)
N_samples = 512
U_net_shape = 1-4-8-16-32-64
batch_size = 8
split_ratio = 0.75
learning_rate = 0.001
files used: 
data_elasticity_3D_128_-1.pt
data_elasticity_3D_128_0.pt
data_elasticity_3D_128_1.pt
data_elasticity_3D_128_2.pt



- circular padding => dry effect ? 
- Cancel oscillation ? (more datas, kernel size,...)
- Visualization with cropped 1/8
- ML Flow for ml flows
- Google cola for computer power
- 512 from data augmented or 512 independant (equilibrium between the two?)
- ratio between (memory size of data, memory size of dataset)
